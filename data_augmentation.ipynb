{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Noise Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This a data augmentation technique where a small amount of random noise is added to the existing data points in a dataset. The purpose of adding noise is to introduce variability to the data while preserving its overall characteristics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"german_credit_prepared.csv\")\n",
    "\n",
    "# Filter data slice\n",
    "good_credit_slice = data[data['credit_history'] == \"all credits at this bank paid back duly\"]\n",
    "\n",
    "# Augment credit amount and duration\n",
    "\n",
    "# augmented_credit_amount = good_credit_slice['credit_amount'] * np.random.uniform(0.9, 1.1, len(good_credit_slice))\n",
    "augmented_duration = good_credit_slice['duration_in_month'] + np.random.randint(-3, 4, len(good_credit_slice))\n",
    "augmented_credit_amount = good_credit_slice['credit_amount'] + np.random.normal(0.0, 1.0, len(good_credit_slice))\n",
    "\n",
    "\n",
    "# Create augmented data\n",
    "augmented_data = good_credit_slice.copy()\n",
    "augmented_data['credit_amount'] = augmented_credit_amount\n",
    "augmented_data['duration_in_month'] = augmented_duration\n",
    "\n",
    "# Concatenate augmented data with original data\n",
    "augmented_data_combined = pd.concat([data, augmented_data], ignore_index=True)\n",
    "\n",
    "# Export \n",
    "augmented_data_combined.to_csv(\"augmented_data_strategy1bis.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 datasets are formed, one using a uniform distribution on the credit amount where the original value is multiplied by a number between 0.9 and 1.1. (augmented_data_strategy1.csv) \n",
    "\n",
    "Uniform distribution - accuracy : 0.7904761904761904 \n",
    "\n",
    "The other one is by adding a white noise to the credit amount. (augmented_data_strategy1bis.csv)\n",
    "\n",
    "Gaussian Distribution - accuracy : 0.7857142857142857\n",
    "\n",
    "However, a better result was achieved by multiplying the original value by the gaussian distribution instead of adding it. The accuracy was 0.8095238095238095.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using now Feature Scaling, a preprocessing technique to standardize or normalize the range of independent variables (features) in the dataset.\n",
    "The main goal is to bring all the features to a similar scale so that they contribute equally to the learning process. There are two common methods of feature scaling: Min-Max Scaling and Standardization.\n",
    "\n",
    "Here, the used method is Min-Max Scaling, it transforms features to a specific range, usually between 0 and 1. It preserves the relationships between the data points by normalizing them.\n",
    "\n",
    "Its formula is X_scaled = (X - X_min) / (X_max - X_min).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = pd.read_csv(\"german_credit_prepared.csv\")\n",
    "\n",
    "# Filter data slices\n",
    "other_slice = data[data['purpose'] == \"Other\"]\n",
    "duration_36_slice = data[data['duration_in_month'] == 36]\n",
    "low_balance_slice = data[data['account_check_status'] == \"<0 DM\"]\n",
    "good_credit_slice = data[data['credit_history'] == \"all credits at this bank paid back duly\"]\n",
    "\n",
    "# Apply feature scaling to credit_amount and age for non-empty slices\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "if not other_slice.empty:\n",
    "    scaled_credit_amount_other = scaler.fit_transform(other_slice[['credit_amount']])\n",
    "    augmented_other_slice = other_slice.copy()\n",
    "    augmented_other_slice['credit_amount'] = scaled_credit_amount_other\n",
    "\n",
    "if not duration_36_slice.empty:\n",
    "    scaled_age_duration_36 = scaler.fit_transform(duration_36_slice[['age']])\n",
    "    augmented_duration_36_slice = duration_36_slice.copy()\n",
    "    augmented_duration_36_slice['age'] = scaled_age_duration_36\n",
    "\n",
    "# Concatenate augmented slices with the original data\n",
    "augmented_data_slices = [data]\n",
    "if not other_slice.empty:\n",
    "    augmented_data_slices.append(augmented_other_slice)\n",
    "if not duration_36_slice.empty:\n",
    "    augmented_data_slices.append(augmented_duration_36_slice)\n",
    "augmented_data_slices.append(low_balance_slice)\n",
    "augmented_data_slices.append(good_credit_slice)\n",
    "augmented_data = pd.concat(augmented_data_slices, ignore_index=True)\n",
    "\n",
    "# Export \n",
    "augmented_data.to_csv(\"augmented_data_strategy3.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy: 0.7709251101321586"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data Augmentation with Random Replacements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique involves randomly replacing categorical feature values with alternative values to create augmented data. It focuses on introducing variability and diversity into the categorical features of a dataset. Categorical features are variables that represent discrete categories or groups; in our case, \"credit_history,\" \"purpose,\" or \"gender\" are such categorical features, but we're going to focus only on \"credit_history\".\n",
    "\n",
    "The alternative values we are going to take should correlate with the original data. For instance, if all credits are paid back duly, we won't choose \"critical account\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"german_credit_prepared.csv\")\n",
    "\n",
    "# Filter data slice\n",
    "credit_history_slice = data[data['credit_history'] == \"all credits at this bank paid back duly\"]\n",
    "\n",
    "# Define the values in the credit_history column\n",
    "credit_history_values = ['existing credits paid back duly till now\t', 'delay in paying off in the past\t', 'no credits taken/ all credits paid back duly', 'all credits at this bank paid back duly']\n",
    "\n",
    "# Augment credit history using random replacements\n",
    "augmented_credit_history = np.random.choice(credit_history_values, size=len(credit_history_slice), replace=True)\n",
    "\n",
    "# Create augmented data\n",
    "augmented_credit_history_slice = credit_history_slice.copy()\n",
    "augmented_credit_history_slice['credit_history'] = augmented_credit_history\n",
    "\n",
    "# Concatenate augmented slice with the original data\n",
    "augmented_data = pd.concat([data, augmented_credit_history_slice], ignore_index=True)\n",
    "\n",
    "\n",
    "# Export\n",
    "augmented_data.to_csv(\"augmented_data_strategy4.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy: 0.7904761904761904"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Noise Addition (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to add random noise on more features this time. The distribution is going to be normal for some and gaussian for others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(\"german_credit_prepared.csv\")\n",
    "\n",
    "# Filter data slices\n",
    "good_credit_slice = data[data['credit_history'] == \"all credits at this bank paid back duly\"]\n",
    "\n",
    "# Augment multiple columns\n",
    "columns_to_augment_normal = ['credit_amount', 'installment_as_income_perc']\n",
    "columns_to_augment_uniform = [ 'duration_in_month',  'age']\n",
    "\n",
    "\n",
    "# Define the range of perturbations for each column\n",
    "perturbation_ranges = {\n",
    "    'credit_amount': (0.9, 1.1),\n",
    "    'duration_in_month': (-3, 4),\n",
    "    'installment_as_income_perc': (0.9, 1.1),\n",
    "    'age': (-2, 3)\n",
    "}\n",
    "\n",
    "# Create augmented data (uniform noise)\n",
    "augmented_data = pd.DataFrame()\n",
    "for column in columns_to_augment_uniform:\n",
    "    perturbation_range = perturbation_ranges[column]\n",
    "    \n",
    "    # Apply perturbations\n",
    "    perturbation_values = np.random.randint(perturbation_range[0], perturbation_range[1], len(good_credit_slice))\n",
    "    augmented_column = good_credit_slice[column] * perturbation_values\n",
    "    \n",
    "    augmented_slice = good_credit_slice.copy()\n",
    "    augmented_slice[column] = augmented_column\n",
    "    \n",
    "    augmented_data = pd.concat([augmented_data, augmented_slice], ignore_index=True)\n",
    "\n",
    "\n",
    "# Create augmented data (Gaussian noise)\n",
    "for column in columns_to_augment_normal:\n",
    "    perturbation_range = perturbation_ranges[column]\n",
    "    \n",
    "    # Apply perturbations\n",
    "    perturbation_values = np.random.normal(perturbation_range[0], perturbation_range[1], len(good_credit_slice))\n",
    "    augmented_column = good_credit_slice[column] * perturbation_values\n",
    "    \n",
    "    augmented_slice = good_credit_slice.copy()\n",
    "    augmented_slice[column] = augmented_column\n",
    "    \n",
    "    augmented_data = pd.concat([augmented_data, augmented_slice], ignore_index=True)\n",
    "\n",
    "# Concatenate augmented data with original data\n",
    "augmented_data_combined = pd.concat([data, augmented_data], ignore_index=True)\n",
    "\n",
    "# Export augmented data to a new CSV file\n",
    "augmented_data_combined.to_csv(\"augmented_data_strategy5.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy : 0.8083333333333333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By duplicating data, we increase the size of the Dataset. This could be good useful if the dataset is small, although it comes with a risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"german_credit_prepared.csv\")\n",
    "\n",
    "# Duplicate the dataset to double its size\n",
    "duplicated_data = pd.concat([data, data], ignore_index=True)\n",
    "\n",
    "# Export\n",
    "duplicated_data.to_csv(\"augmented_data_strategy6.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy : 0.765\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling + Random Noise Addition + Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are combining two of the most fruitful methods we used that can be mixed without much difficulty. After that, the dataset will get shuffled to prevent any inherent pattern and to reduce possible bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"german_credit_prepared.csv\")\n",
    "\n",
    "\n",
    "# Filter data slices\n",
    "other_slice = data[data['purpose'] == \"Other\"]\n",
    "duration_36_slice = data[data['duration_in_month'] == 36]\n",
    "low_balance_slice = data[data['account_check_status'] == \"<0 DM\"]\n",
    "good_credit_slice = data[data['credit_history'] == \"all credits at this bank paid back duly\"]\n",
    "\n",
    "# Apply feature scaling to credit_amount and age for non-empty slices\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "if not other_slice.empty:\n",
    "    scaled_credit_amount_other = scaler.fit_transform(other_slice[['credit_amount']])\n",
    "    scaked_other_slice = other_slice.copy()\n",
    "    scaled_other_slice['credit_amount'] = scaled_credit_amount_other\n",
    "\n",
    "if not duration_36_slice.empty:\n",
    "    scaled_age_duration_36 = scaler.fit_transform(duration_36_slice[['age']])\n",
    "    scaled_duration_36_slice = duration_36_slice.copy()\n",
    "    scaled_duration_36_slice['age'] = scaled_age_duration_36\n",
    "\n",
    "# Concatenate augmented slices with the original data\n",
    "scaled_data_slices = [data]\n",
    "if not other_slice.empty:\n",
    "    scaled_data_slices.append(scaled_other_slice)\n",
    "if not duration_36_slice.empty:\n",
    "    scaled_data_slices.append(scaled_duration_36_slice)\n",
    "scaled_data_slices.append(low_balance_slice)\n",
    "scaled_data_slices.append(good_credit_slice)\n",
    "scaled_data = pd.concat(scaled_data_slices, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# Augment multiple columns\n",
    "columns_to_augment_normal = ['credit_amount', 'installment_as_income_perc']\n",
    "columns_to_augment_uniform = [ 'duration_in_month',  'age']\n",
    "\n",
    "\n",
    "# Define the range of perturbations for each column\n",
    "perturbation_ranges = {\n",
    "    'credit_amount': (0.9, 1.1),\n",
    "    'duration_in_month': (-3, 4),\n",
    "    'installment_as_income_perc': (0.9, 1.1),\n",
    "    'age': (-2, 3)\n",
    "}\n",
    "\n",
    "good_credit_slice = scaled_data[scaled_data['credit_history'] == \"all credits at this bank paid back duly\"]\n",
    "\n",
    "\n",
    "# Create augmented data (uniform noise)\n",
    "augmented_data = pd.DataFrame()\n",
    "for column in columns_to_augment_uniform:\n",
    "    perturbation_range = perturbation_ranges[column]\n",
    "    \n",
    "    # Apply perturbations\n",
    "    perturbation_values = np.random.randint(perturbation_range[0], perturbation_range[1], len(good_credit_slice))\n",
    "    augmented_column = good_credit_slice[column] * perturbation_values\n",
    "    \n",
    "    augmented_slice = good_credit_slice.copy()\n",
    "    augmented_slice[column] = augmented_column\n",
    "    \n",
    "    augmented_data = pd.concat([augmented_data, augmented_slice], ignore_index=True)\n",
    "\n",
    "\n",
    "# Create augmented data (Gaussian noise)\n",
    "for column in columns_to_augment_normal:\n",
    "    perturbation_range = perturbation_ranges[column]\n",
    "    \n",
    "    # Apply perturbations\n",
    "    perturbation_values = np.random.normal(perturbation_range[0], perturbation_range[1], len(good_credit_slice))\n",
    "    augmented_column = good_credit_slice[column] * perturbation_values\n",
    "    \n",
    "    augmented_slice = good_credit_slice.copy()\n",
    "    augmented_slice[column] = augmented_column\n",
    "    \n",
    "    augmented_data = pd.concat([augmented_data, augmented_slice], ignore_index=True)\n",
    "\n",
    "# Concatenate augmented data with original data\n",
    "augmented_data_combined = pd.concat([data, augmented_data], ignore_index=True)\n",
    "\n",
    "# Shuffle the augmented dataset\n",
    "shuffled_augmented_data = augmented_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Export the shuffled augmented data to a new CSV file\n",
    "shuffled_augmented_data.to_csv(\"augmented_data_strategy7.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model a few times with augmented datasets using this method, we can get these accuracies:    \n",
    "0.9625   \n",
    "0.975       \n",
    "0.9375  \n",
    "0.95    \n",
    "which is by far better than the initial 0.755 we had with the original dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
